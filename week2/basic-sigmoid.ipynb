{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC SIGMOID USING 'MATH' LIBRARY\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x == A scalar\n",
    "    \n",
    "    Return: \n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-81efe0d53f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbasic_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-45f09df75363>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "basic_sigmoid(x)\n",
    "#this errors because 'math' library uses real number\n",
    "#this is why we use numpy because we mostly use matrices and vectors in deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "\n",
    "print(np.exp(x))\n",
    "print(x + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID USING NUMPY\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGMOID GRADIENT\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "        Compute the gradient (also called the slop or derivative) of the sigmoid function with respect to its input x.\n",
    "    \n",
    "        Arguments:\n",
    "        x -- A scalar or numpy array\n",
    "    \n",
    "        Return:\n",
    "        ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    s = sigmoid(x)\n",
    "    \n",
    "    ds = s*(1-s)\n",
    "    \n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[0.73105858 0.88079708 0.95257413]\n",
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "# from above: x = np.array([1,2,3])\n",
    "\n",
    "print(\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING WITH IMAGE VECTORS\n",
    "\n",
    "\"\"\"\n",
    "images are represented by a 3D array of shape(length, height, depth = 3) but when you read an image as the input \n",
    "of an algorithm you convert it to a vector of shape(length*height*3,1).  In other words, you unroll or reshape it\n",
    "into a 1D vector\n",
    "\"\"\"\n",
    "\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth,1)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(image.shape)\n",
    "    \n",
    "    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2)\n",
      "iamge2vector:: [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "pic1 = np.array([\n",
    "    [[ 0.67826139,  0.29380381],\n",
    "     [ 0.90714982,  0.52835647],\n",
    "     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "    [[ 0.92814219,  0.96677647],\n",
    "     [ 0.85304703,  0.52351845],\n",
    "     [ 0.19981397,  0.27417313]],\n",
    "\n",
    "    [[ 0.60659855,  0.00533165],\n",
    "     [ 0.10820313,  0.49978937],\n",
    "     [ 0.34144279,  0.94630077]]\n",
    "    ])\n",
    "\n",
    "\n",
    "print(\"iamge2vector:: \" + str(image2vector(pic1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZING ROWS\n",
    "\"\"\"\n",
    "normalizing data leads to better performance because gradient descent converges faster after normalization\n",
    "This is done be dividing each row vector of x by its norm\n",
    "\"\"\"\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns: \n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x\n",
    "    \"\"\"\n",
    "    \n",
    "    x_norm = np.linalg.norm(x, ord = 2, axis = 1, keepdims = True)\n",
    "    \n",
    "    x = x / x_norm\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "newX = np.array([\n",
    "    [0,3,4],\n",
    "    [1,6,4]\n",
    "])\n",
    "\n",
    "print(normalizeRows(newX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BROADCASTING AND SOFTMAX FUNCTION\n",
    "\n",
    "# think of softmax as a normalizing function used when your algorithm needs to classify two or more classes\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    code should work for both row vector and matrices of shape (n, m)\n",
    "    \n",
    "    Argument: \n",
    "    x -- A numpy matrix of shape (n,m)\n",
    "    \n",
    "    Returns: \n",
    "    s -- A numpy matrix equal to the softmax of x, of shape(n,m)\n",
    "    \"\"\"\n",
    "    \n",
    "    #apply exp() element-wise to x\n",
    "    x_exp = np.exp(x)\n",
    "    \n",
    "    #create a vector x_sum that sums each row of x_exp\n",
    "    x_sum = np.sum(x_exp, axis = 1, keepdims = True)\n",
    "    \n",
    "    #compute softmax(x) by dividing x_exp by x_sum.\n",
    "    s = x_exp / x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9,2,5,0,0],\n",
    "    [7,5,0,0,0]\n",
    "])\n",
    "\n",
    "print(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTIONS\n",
    "\n",
    "\"\"\"\n",
    "the loss function is used to evaluate the performance of your model.  The bigger\n",
    "your loss is, the more different your predictions (yhat) are from the true values (y).\n",
    "In deep learning you use optimization algorithms like Gradient Descent ot train your model and to minimze the cost\n",
    "\"\"\"\n",
    "\n",
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = np.sum(np.abs(yhat-y),axis = 0)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(L1(yhat, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
